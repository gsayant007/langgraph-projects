{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cf6e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import (\n",
    "    KnowledgeGraphIndex, ServiceContext,\n",
    "    SimpleDirectoryReader, StorageContext, Document)\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6afee4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 46.99it/s]\n",
      "Processing nodes:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 14:15:33,118 - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:15:55,886 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:15:56,314 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:15:56,411 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:15:56,512 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:15:56,610 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  5.53it/s]\n",
      "Processing nodes:   8%|▊         | 1/12 [00:23<04:20, 23.66s/it]2025-12-22 14:16:19,194 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:16:19,315 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:16:19,402 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:16:19,490 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 10.50it/s]\n",
      "Processing nodes:  17%|█▋        | 2/12 [00:46<03:51, 23.20s/it]2025-12-22 14:16:41,163 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:16:41,277 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:16:41,359 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 10.25it/s]\n",
      "Processing nodes:  25%|██▌       | 3/12 [01:08<03:23, 22.59s/it]2025-12-22 14:17:03,979 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:17:04,077 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:17:04,162 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:17:04,240 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:17:04,322 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00, 11.71it/s]\n",
      "Processing nodes:  33%|███▎      | 4/12 [01:31<03:01, 22.74s/it]2025-12-22 14:17:25,944 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Processing nodes:  42%|████▏     | 5/12 [01:52<02:36, 22.34s/it]2025-12-22 14:17:49,070 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:17:49,172 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:17:49,256 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:17:49,353 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 10.63it/s]\n",
      "Processing nodes:  50%|█████     | 6/12 [02:16<02:16, 22.70s/it]2025-12-22 14:18:10,048 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:18:10,150 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:18:10,234 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:18:10,321 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 11.03it/s]\n",
      "Processing nodes:  58%|█████▊    | 7/12 [02:37<01:50, 22.13s/it]2025-12-22 14:18:32,110 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:18:32,225 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:18:32,312 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:18:32,389 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 10.80it/s]\n",
      "Processing nodes:  67%|██████▋   | 8/12 [02:59<01:28, 22.11s/it]2025-12-22 14:18:54,656 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:18:54,777 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:18:54,859 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:18:54,937 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:18:55,015 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00, 11.21it/s]\n",
      "Processing nodes:  75%|███████▌  | 9/12 [03:22<01:06, 22.27s/it]2025-12-22 14:19:17,360 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Processing nodes:  83%|████████▎ | 10/12 [03:44<00:44, 22.30s/it]2025-12-22 14:19:39,784 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:19:39,883 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:19:39,963 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:19:40,045 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:19:40,124 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00, 11.83it/s]\n",
      "Processing nodes:  92%|█████████▏| 11/12 [04:07<00:22, 22.44s/it]2025-12-22 14:20:01,578 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:20:01,687 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:20:01,766 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 10.69it/s]\n",
      "Processing nodes: 100%|██████████| 12/12 [04:28<00:00, 22.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_CBEventType.NODE_PARSING -> 0.02347 seconds\n",
      "      |_CBEventType.CHUNKING -> 0.020712 seconds\n",
      "    |_CBEventType.TEMPLATING -> 3.1e-05 seconds\n",
      "    |_CBEventType.LLM -> 22.767181 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.721245 seconds\n",
      "    |_CBEventType.TEMPLATING -> 2.5e-05 seconds\n",
      "    |_CBEventType.LLM -> 22.5841 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.283967 seconds\n",
      "    |_CBEventType.TEMPLATING -> 3e-05 seconds\n",
      "    |_CBEventType.LLM -> 21.669465 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.193459 seconds\n",
      "    |_CBEventType.TEMPLATING -> 2.6e-05 seconds\n",
      "    |_CBEventType.LLM -> 22.616994 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.339952 seconds\n",
      "    |_CBEventType.TEMPLATING -> 2.6e-05 seconds\n",
      "    |_CBEventType.LLM -> 21.619256 seconds\n",
      "    |_CBEventType.TEMPLATING -> 4e-05 seconds\n",
      "    |_CBEventType.LLM -> 23.120903 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.280436 seconds\n",
      "    |_CBEventType.TEMPLATING -> 2.5e-05 seconds\n",
      "    |_CBEventType.LLM -> 20.69109 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.270248 seconds\n",
      "    |_CBEventType.TEMPLATING -> 2.5e-05 seconds\n",
      "    |_CBEventType.LLM -> 21.786797 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.276122 seconds\n",
      "    |_CBEventType.TEMPLATING -> 2.2e-05 seconds\n",
      "    |_CBEventType.LLM -> 22.264054 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.354616 seconds\n",
      "    |_CBEventType.TEMPLATING -> 2.6e-05 seconds\n",
      "    |_CBEventType.LLM -> 22.341648 seconds\n",
      "    |_CBEventType.TEMPLATING -> 3.2e-05 seconds\n",
      "    |_CBEventType.LLM -> 22.419632 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.336513 seconds\n",
      "    |_CBEventType.TEMPLATING -> 3.5e-05 seconds\n",
      "    |_CBEventType.LLM -> 21.451397 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.185198 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 14:20:04,498 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:20:04,552 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:20:04,555 - INFO - > Querying with idx: f365a57a-e55a-4432-8393-8a5a40fbdc72: Statistics indicate that animal lovers in recent years have shown a preferenc...\n",
      "2025-12-22 14:20:04,555 - INFO - > Querying with idx: 237a05d2-004c-4347-81cb-f341364e94e6: In fact, it will give them diarrhea. Instead, give your cat cream. The butter...\n",
      "2025-12-22 14:20:04,556 - INFO - > Querying with idx: 64e484f6-21d2-4697-826d-6d176cfb3fe8: There is no single cat called the panther. The name is commonly applied to th...\n",
      "2025-12-22 14:20:04,557 - INFO - > Querying with idx: 96d2418d-0a81-4655-9733-b05334400f83: Cats’ hearing stops at 65 khz (kilohertz); humans’ hearing stops at 20 khz.\n",
      "I...\n",
      "2025-12-22 14:20:04,557 - INFO - > Querying with idx: fd1fa4ae-e733-4e86-b25f-1b1b91d22d67: Most cats had short hair until about 100 years ago, when it became fashionabl...\n",
      "2025-12-22 14:20:04,557 - INFO - > Querying with idx: c510fe34-7aac-46cb-8ad9-ce74f106f38e: On average, cats spend 2/3 of every day sleeping. That means a nine-year-old ...\n",
      "2025-12-22 14:22:00,272 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY -> 118.445046 seconds\n",
      "      |_CBEventType.RETRIEVE -> 2.729246 seconds\n",
      "        |_CBEventType.TEMPLATING -> 2.4e-05 seconds\n",
      "        |_CBEventType.LLM -> 2.66929 seconds\n",
      "        |_CBEventType.EMBEDDING -> 0.054163 seconds\n",
      "      |_CBEventType.SYNTHESIZE -> 115.715283 seconds\n",
      "        |_CBEventType.TEMPLATING -> 1.8e-05 seconds\n",
      "        |_CBEventType.LLM -> 115.708862 seconds\n",
      "**********\n",
      "Cats have true fur, in that they have both an undercoat and an outer coat.\n"
     ]
    }
   ],
   "source": [
    "#setup api key\n",
    "# load_dotenv()\n",
    "# Initialize the 1b model for general tasks\n",
    "llm = Ollama(model=\"gemma3:1b\", \n",
    "             temperature=0.0,\n",
    "             request_timeout=300,keep_alive=\"10m\")\n",
    "\n",
    "# Set the embedding model (Required for 'hybrid' mode)\n",
    "# Make sure you have run 'ollama pull nomic-embed-text'\n",
    "embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "# Set these globally for LlamaIndex to use\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "#load documents\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "# model = ChatOllama(\n",
    "# model=\"gemma3:1b\",\n",
    "# temperature=0.5,\n",
    "# )\n",
    "# messages = [\n",
    "#     (\"system\",\"You are a helpful agent\"),\n",
    "#     (\"human\",\"Hi, hello!\")\n",
    "# ]\n",
    "# response = model.invoke(messages)\n",
    "# print(response.content,end=\"\\n\")\n",
    "\n",
    "#Initialize the Graph Store\n",
    "graph_store = SimpleGraphStore()\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "#build the knowledge graph  index, this step uses LLM to extract entities and relationships\n",
    "index = KnowledgeGraphIndex.from_documents(\n",
    "documents,\n",
    "max_triplets_per_chunk=2,\n",
    "storage_context=storage_context,\n",
    "include_embeddings = True,\n",
    "show_progress=True,\n",
    ")\n",
    "\n",
    "# Query the Graph\n",
    "query_engine = index.as_query_engine(\n",
    "include_text=True,\n",
    "response_mode=\"tree_summarize\",\n",
    "embedding_mode=\"hybrid\", # uses both vector and graph search\n",
    "similarity_top_k=5,\n",
    ")\n",
    "response = query_engine.query(\"Describe a cat\")\n",
    "print(response,end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d259d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# from openai import OpenAI\n",
    "# from datetime import datetime, timezone\n",
    "\n",
    "# # Set your key in the environment before running, e.g.:\n",
    "# #   export OPENAI_API_KEY=\"sk-...\"\n",
    "# # or set it here (NOT recommended to hardcode secrets)\n",
    "# # os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# def list_models():\n",
    "#     client = OpenAI()  # uses OPENAI_API_KEY from env\n",
    "#     models = client.models.list()\n",
    "\n",
    "#     # Convert to a friendly, sorted listing\n",
    "#     items = sorted(models.data, key=lambda m: m.id)\n",
    "#     print(f\"Found {len(items)} models @ {datetime.now(timezone.utc).isoformat()}\")\n",
    "#     for m in items:\n",
    "#         # Not all fields are guaranteed; guard with getattr\n",
    "#         owned_by = getattr(m, \"owned_by\", \"unknown\")\n",
    "#         created = getattr(m, \"created\", None)\n",
    "#         created_iso = (\n",
    "#             datetime.fromtimestamp(created, tz=timezone.utc).isoformat()\n",
    "#             if isinstance(created, int) else \"n/a\"\n",
    "#         )\n",
    "#         print(f\"- id: {m.id:20} | owner: {owned_by:15} | created: {created_iso}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         list_models()\n",
    "#     except Exception as e:\n",
    "#         print(\"Error listing models:\", e)\n",
    "#         print(\"• Ensure OPENAI_API_KEY is set\")\n",
    "#         print(\"• Check network access and organization/project permissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a353f02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 14:09:48,737 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:09:52,919 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:09:57,016 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:10:00,928 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:10:04,883 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:10:08,960 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:10:12,918 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:10:16,952 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:10:21,008 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:10:25,119 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:10:29,168 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:10:32,878 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_CBEventType.NODE_PARSING -> 0.022981 seconds\n",
      "      |_CBEventType.CHUNKING -> 0.021858 seconds\n",
      "    |_CBEventType.EMBEDDING -> 41.057406 seconds\n",
      "    |_CBEventType.EMBEDDING -> 7.758074 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 14:10:33,042 - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector index response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 14:10:33,103 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 14:11:31,729 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY -> 58.686568 seconds\n",
      "      |_CBEventType.RETRIEVE -> 0.061787 seconds\n",
      "        |_CBEventType.EMBEDDING -> 0.059506 seconds\n",
      "      |_CBEventType.SYNTHESIZE -> 58.6242 seconds\n",
      "        |_CBEventType.TEMPLATING -> 1.6e-05 seconds\n",
      "        |_CBEventType.LLM -> 58.617289 seconds\n",
      "**********\n",
      "Cats are domesticated felines known for their independence, agility, and unique behaviors. They are members of the Felidae family and exhibit a remarkable array of traits, including a strong sense of territory, excellent hearing, and a distinctive purr. Cats are often portrayed as mysterious and independent creatures, though they are also beloved companions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Setup ---\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Optional: better logs to see what's happening\n",
    "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n",
    "\n",
    "debug_handler = LlamaDebugHandler(print_trace_on_end=True)\n",
    "cb_manager = CallbackManager([debug_handler])\n",
    "\n",
    "# 1) Use a valid Ollama model and a sane timeout\n",
    "llm = Ollama(\n",
    "    model=\"gemma3:1b\",           # or \"llama3.1:8b\" if you have it pulled\n",
    "    request_timeout=120,         # increase for first-run warmup\n",
    "    # base_url=\"http://localhost:11434\",  # set if your Ollama runs elsewhere\n",
    ")\n",
    "\n",
    "# 2) Embeddings (hybrid needs this later anyway)\n",
    "embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.callback_manager = cb_manager\n",
    "Settings.chunk_size = 1024  # moderate chunking\n",
    "\n",
    "# 3) Load a small test document to verify path\n",
    "documents = SimpleDirectoryReader(input_dir=\"./data\").load_data()\n",
    "# Ensure ./data has at least one small .txt/.md/.pdf\n",
    "\n",
    "# 4) Build a vector index first (quick, reliable)\n",
    "v_index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 5) Query (should return quickly and consistently)\n",
    "v_engine = v_index.as_query_engine(similarity_top_k=3)\n",
    "print(\"Vector index response:\")\n",
    "print(v_engine.query(\"Describe a cat\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8eb132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
